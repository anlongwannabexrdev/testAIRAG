import faiss
import numpy as np
import json
from ollama import Client

# 1. Khá»Ÿi táº¡o client Ollama
ollama = Client()

# 2. Load file .json
with open("knowledge.json", "r", encoding="utf-8") as f:
    data = json.load(f)

documents = [item["text"] for item in data["items"]]
print("Loaded", len(documents), "documents from knowledge.json")

# 3. HÃ m táº¡o embedding vá»›i nomic-embed-text
def get_embedding(text):
    res = ollama.embed(model="nomic-embed-text", input=text)
    return np.array(res["embeddings"][0], dtype="float32")

# 4. Táº¡o index FAISS
dimension = len(get_embedding("test"))  # kÃ­ch thÆ°á»›c vector tá»« model
index = faiss.IndexFlatL2(dimension)

for doc in documents:
    emb = get_embedding(doc)
    index.add(np.array([emb]))

print("âœ… ÄÃ£ build xong FAISS index.")

# HÃ m rerank báº±ng LLM
def rerank(query, candidates):
    context = "\n".join([f"-{c}" for c in candidates])
    prompt= f"""
    CÃ¢u há»i: {query}

    CÃ¡c Ä‘oáº¡n trÃ­ch dÆ°á»›i Ä‘Ã¢y Ä‘Æ°á»£c láº¥y tá»« knowledge.json:
    {context}

    Nhiá»‡m vá»¥: HÃ£y CHá»ŒN ra nhá»¯ng Ä‘oáº¡n LIÃŠN QUAN NHáº¤T Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i.
    Tráº£ vá» nguyÃªn vÄƒn Ä‘oáº¡n vÄƒn báº£n phÃ¹ há»£p nháº¥t (1â€“2 Ä‘oáº¡n). 
    Náº¿u khÃ´ng cÃ³ Ä‘oáº¡n nÃ o liÃªn quan, tráº£ vá»: "KhÃ´ng tÃ¬m tháº¥y".
    """

    res = ollama.chat(
        model="gpt-oss:20b",
        messages=[{"role": "user", "content": prompt}]
    )
    return res["message"]["content"]

# 5. HÃ m truy váº¥n RAG
def rag_query(query, top_k=2):
    q_emb = get_embedding(query)
    D, I = index.search(np.array([q_emb]), top_k)
    candidates = [documents[i] for i in I[0]]

    # DÃ¹ng LLM lá»c láº¡i
    filtered_context = rerank(query, candidates)

    final_prompt = f"""
    Báº¡n lÃ  má»™t trá»£ lÃ½ chá»‰ Ä‘Æ°á»£c phÃ©p tráº£ lá»i dá»±a trÃªn THÃ”NG TIN THAM CHIáº¾U bÃªn dÆ°á»›i.
    Náº¿u khÃ´ng cÃ³ thÃ´ng tin liÃªn quan, hÃ£y tráº£ lá»i: "KhÃ´ng tÃ¬m tháº¥y trong dá»¯ liá»‡u."

    CÃ¢u há»i: {query}
    ThÃ´ng tin tham chiáº¿u:
    {filtered_context}

    HÃ£y tráº£ lá»i dá»±a trÃªn thÃ´ng tin trÃªn.
    """
    response = ollama.chat(
        model="gpt-oss:20b",
        messages=[{"role": "user", "content": final_prompt}]
    )
    return response["message"]["content"]

while True:
    query = input("Nháº­p prompt ('exit' Ä‘á»ƒ thoÃ¡t): ")
    if query.lower() == "exit":
        break

    answer = rag_query(query)
    print("ğŸ’¡ Answer:", answer)
